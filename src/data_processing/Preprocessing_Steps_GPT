1ï¸âƒ£ Fehlende Werte behandeln

Warum: LSTM kann NaNs nicht verarbeiten. Jede fehlende Zahl bricht das Training.

Was tun:

Zeilen lÃ¶schen

oder Werte auffÃ¼llen (z.â€¯B. Mittelwert, Median, Forward/Backward Fill)

Alternativen: Interpolation (linear, spline), Rolling-Werte, domain-spezifische FÃ¼llung

â¡ï¸ Tipp zum Start: PrÃ¼fe zuerst, wie viele NaNs pro Spalte vorhanden sind, um zu entscheiden, welche Strategie sinnvoll ist.




2ï¸âƒ£ Feature-Auswahl

Warum: LSTM lernt besser, wenn irrelevante Spalten (IDs, Zeitstempel, Kategorien) entfernt werden.

Was tun:

Sensorwerte isolieren

Zeitinformationen ggf. in Sinus/Cosinus umwandeln (z.â€¯B. fÃ¼r Zykluszeiten)

Alternativen: Feature-Engineering: differenzen der Sensorwerte, gleitender Mittelwert, Ratios

â¡ï¸ Tipp: Beginne mit den wichtigsten kontinuierlichen Sensorwerten, bevor du komplexes Engineering machst.




3ï¸âƒ£ Normalisierung / Skalierung

Warum: LSTM-Training wird instabil, wenn Features stark unterschiedliche Skalen haben.

Was tun:

MinMax-Scaling (0â€“1)

Standardisierung (mean=0, std=1)

Alternativen: RobustScaler (gegen AusreiÃŸer), Log-Transformation bei stark schiefen Verteilungen

â¡ï¸ Tipp: Trainiere den Scaler nur auf Trainingsdaten und wende ihn auf Testdaten an.




4ï¸âƒ£ Sequenzbildung

Warum: LSTM verarbeitet zeitliche AbhÃ¤ngigkeiten â€“ du musst also Sequenzen aus den Rohdaten bauen.

Was tun:

Fenster Ã¼ber die Zeit (z.â€¯B. 30 Zeitschritte)

Jede Sequenz wird Input und Label fÃ¼r Autoencoder sein (bei Anomaly Detection meist Input=Label)

Alternativen: Variable SequenzlÃ¤nge, Sliding Window mit Schrittweite > 1

â¡ï¸ Tipp: Experimentiere mit verschiedenen SequenzlÃ¤ngen â€“ zu kurz â†’ kein Kontext, zu lang â†’ Training wird langsamer.




5ï¸âƒ£ Umgang mit AusreiÃŸern

Warum: Extreme Werte kÃ¶nnen LSTM-Ausgaben verzerren.

Was tun:

Clippen

Entfernen

RobustScaler verwenden

Alternativen: Winsorization, Transformieren (z.â€¯B. log)

â¡ï¸ Tipp: Schau dir die Verteilungen einzelner Sensoren an â€“ oft fallen AusreiÃŸer sofort auf.




6ï¸âƒ£ Optional: Datenaugmentation

Warum: Mehr Daten = stabileres Modell.

Was tun:

Kleine Zufallsrauschen hinzufÃ¼gen

Zeitliche Verschiebungen

Alternativen: SMOTE-artige Methoden fÃ¼r Zeitreihen, Synthetische Sequenzen




7ï¸âƒ£ Optional: Feature-Engineering

Warum: LSTM lernt besser mit sinnvollen Features.

Was tun:

Gleitende Mittelwerte / Rolling Stats

Differenzen zwischen aufeinanderfolgenden Werten

Kombination mehrerer Sensoren

Empfohlene Reihenfolge zum Implementieren

Fehlende Werte behandeln â†’ unbedingt zuerst

Feature-Auswahl â†’ IDs, Zeitstempel raus

Skalierung / Normalisierung

Sequenzbildung fÃ¼r LSTM

AusreiÃŸerbehandlung (vor oder nach Skalierung mÃ¶glich)

Optional: Datenaugmentation

Optional: Feature-Engineering





ğŸ’¡ Merksatz:

â€Erst Daten sauber machen, dann skalieren, dann Sequenzen bauen, dann experimentieren.â€œ